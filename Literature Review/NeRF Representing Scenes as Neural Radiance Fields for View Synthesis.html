<!DOCTYPE html>
<html>
<head>
<title>NeRF Representing Scenes as Neural Radiance Fields for View Synthesis.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
	*  Copyright (c) Microsoft Corporation. All rights reserved.
	*  Licensed under the MIT License. See License.txt in the project root for license information.
	*--------------------------------------------------------------------------------------------*/

a { color: rgb(213, 213, 213);
}
	
body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
	margin: 3% 0px 5% 0px;
	color: rgb(213, 213, 213);
	background-color: rgb(39, 37, 37);
}

iframe { 
	display: block;
	margin: 0 auto;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
	*  Copyright (c) Microsoft Corporation. All rights reserved.
	*  Licensed under the MIT License. See License.txt in the project root for license information.
	*--------------------------------------------------------------------------------------------*/

a { color: rgb(213, 213, 213);
}
	
body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
	margin: 3% 0px 5% 0px;
	color: rgb(213, 213, 213);
	background-color: rgb(39, 37, 37);
}

iframe { 
	display: block;
	margin: 0 auto;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
	*  Copyright (c) Microsoft Corporation. All rights reserved.
	*  Licensed under the MIT License. See License.txt in the project root for license information.
	*--------------------------------------------------------------------------------------------*/

a { color: rgb(213, 213, 213);
}
	
body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
	margin: 3% 0px 5% 0px;
	color: rgb(213, 213, 213);
	background-color: rgb(39, 37, 37);
}

iframe { 
	display: block;
	margin: 0 auto;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
	*  Copyright (c) Microsoft Corporation. All rights reserved.
	*  Licensed under the MIT License. See License.txt in the project root for license information.
	*--------------------------------------------------------------------------------------------*/

a { color: rgb(213, 213, 213);
}
	
body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
	margin: 3% 0px 5% 0px;
	color: rgb(213, 213, 213);
	background-color: rgb(39, 37, 37);
}

iframe { 
	display: block;
	margin: 0 auto;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
	*  Copyright (c) Microsoft Corporation. All rights reserved.
	*  Licensed under the MIT License. See License.txt in the project root for license information.
	*--------------------------------------------------------------------------------------------*/

a { color: rgb(213, 213, 213);
}
	
body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
	margin: 3% 0px 5% 0px;
	color: rgb(213, 213, 213);
	background-color: rgb(39, 37, 37);
}

iframe { 
	display: block;
	margin: 0 auto;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
	* Markdown PDF CSS
	*/

	body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
	color: rgb(213, 213, 213);
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}
	
.squash {
margin: 0px 18% 0px 18%;
}
</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>

<div class="squash">
<p><strong>October 16, 2023</strong></p>
<h1 id="concept-lookups-from-nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">Concept Lookups from <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em></h1>
<p>Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., &amp; Ng, R. (2020). NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. Lecture Notes in Computer Science. https://doi.org/10.1007/978-3-030-58452-8_24</p>
<h3 id="table-of-contents">Table of Contents</h3>
<ol>
<li><a href="#cvsf">Continuous volumetric scene function</a></li>
<li><a href="#volurend">Volumetric rendering</a></li>
<li><a href="#cvrt">Classic volumetric rendering techniques</a></li>
<li><a href="#viewsynth">View synthesis</a></li>
<li><a href="#fcnn">Deep fully-connected neural network</a></li>
<li><a href="#conlay">Convolutional layers</a></li>
<li><a href="#cnn">Convolutional neural networks</a></li>
<li><a href="#mlp">Multilayer perceptron</a></li>
<li><a href="#mcrtas">Marching camera rays through a scene</a></li>
<li><a href="#gradesc">Gradient descent</a></li>
<li><a href="#posenc">Positional encoding</a></li>
<li><a href="#sdf">Signed distance functions</a></li>
<li><a href="#diffrast">Differentiable rasterizers</a></li>
<li><a href="#volrep">Volumetric representation</a></li>
<li><a href="#quadrature">Quadrature</a></li>
<li><a href="#sampling">Stratified sampling</a></li>
<li><a href="#pdc">Alpha compositing (Porter-Duff) model</a></li>
<li><a href="#lamb">Lambertian objects</a></li>
<li><a href="#psnr">Peak signal-to-noise ratio (PSNR)</a></li>
<li><a href="#ssim">Structural similarity index (SSIM)</a></li>
<li><a href="#lpips">Learned perceptual image patch similarity (LPIPS)</a></li>
<li><a href="#diffobj">Diffuse objects</a></li>
<li><a href="#forward">Forward facing captures</a></li>
<li><a href="#srn">Scene representation networks (SRN)</a></li>
<li><a href="#llff">Local Light Field Fusion (LLFF)</a></li>
<li><a href="#l3vg">Large 3D voxel grids</a></li>
<li><a href="#relu">ReLU activations (ReLU) / Activation functions</a></li>
<li><a href="#sigmoid">Sigmoid activatoin</a></li>
<li><a href="#ndc">Normalized Device Coordinate (NDC)</a></li>
<li><a href="#trirast">Triangle rasterization</a></li>
<li><a href="#lid">&quot;Linear in disparity&quot;</a></li>
<li><a href="#raymap">&quot;rays to map them from camera space to NDC space&quot;</a></li>
<li><a href="#ovf">Original viewing frustum</a></li>
<li><a href="#spcm">Standard pinhole camera model</a></li>
<li><a href="#tmm">Texture-mapped meshes</a></li>
</ol>
<hr>
<h3 id="1-continuous-volumetric-scene-function-a-name%22cvsf%22a">1. Continuous volumetric scene function <a name="cvsf"></a></h3>
<p>from ChatGPT, &quot;A continuous volumetric scene function is a mathematical representation or model used in computer graphics and computer vision to describe the visual properties of a 3D scene in a continuous and volumetric manner. In simpler terms, it's a way to mathematically represent what a 3D scene looks like and how it behaves in a continuous and spatially coherent manner.</p>
<p>This concept is often associated with techniques like volumetric rendering and voxel-based representations. Here are some key points about continuous volumetric scene functions:</p>
<ol>
<li>
<p><strong>Volumetric Representation</strong>: Instead of relying on traditional surface-based models (like 3D mesh models), continuous volumetric scene functions describe the contents of a 3D scene throughout the entire volume. This allows for more accurate and detailed representations of the scene, including not just the surfaces but also the interiors.</p>
</li>
<li>
<p><strong>Spatial Coherence</strong>: These functions provide spatial coherence, meaning that they describe how the properties of the scene vary continuously in 3D space. This is especially useful for representing phenomena such as scattering, lighting, and materials that don't fit neatly into surface-based models.</p>
</li>
<li>
<p><strong>Integration with Ray Tracing</strong>: Continuous volumetric scene functions can be used with ray tracing techniques to simulate how light interacts with the scene. This is particularly valuable for creating realistic images with complex lighting and shading effects.</p>
</li>
<li>
<p><strong>Applications</strong>: They are used in various applications, such as computer graphics (for rendering realistic scenes), medical imaging (for 3D visualization of medical data), and computer vision (for understanding 3D scenes from images or video).</p>
</li>
</ol>
<p>To create a continuous volumetric scene function, you often need to sample and store data at various points within the 3D space. This data may include information about materials, colors, density, lighting, and other properties. Techniques for creating and using these functions can vary, but the general idea is to create a detailed and coherent representation of the 3D scene that can be used for rendering or analysis.&quot;</p>
<h3 id="2-volumetric-rendering-a-name%22volurend%22a">2. Volumetric rendering <a name="volurend"></a></h3>
<iframe width="560" 
    height="315" 
    src="https://www.youtube.com/embed/Fd97_4NY9Uw?si=VvFfjoBQurrrQEpc" 
    title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    allowfullscreen>
</iframe>
<br>
<h3 id="3-classic-volume-rendering-techniques-a-name%22cvrt%22a">3. Classic volume rendering techniques <a name="cvrt"></a></h3>
<p>from ChatGPT, &quot;Classic volume rendering techniques are methods used to visualize and explore 3D volumetric data, such as medical images (e.g., CT and MRI scans), scientific simulations, and industrial data. These techniques help in creating 2D representations that convey meaningful information about the internal structures and properties of the data. Some of the classic volume rendering techniques include:</p>
<ol>
<li>
<p><strong>Maximum Intensity Projection (MIP)</strong>: MIP projects the maximum voxel intensity along each viewing ray onto the 2D image. It is useful for highlighting high-intensity structures, such as bones in medical imaging.</p>
</li>
<li>
<p><strong>Minimum Intensity Projection (MinIP)</strong>: MinIP projects the minimum voxel intensity along each viewing ray. It is useful for highlighting low-intensity features, like airways in medical imaging.</p>
</li>
<li>
<p><strong>Average Intensity Projection (AIP)</strong>: AIP computes the average intensity along each viewing ray and projects it onto the 2D image. It provides a smoothed representation of the volume data.</p>
</li>
<li>
<p><strong>Compositing Techniques</strong>: These include techniques like direct volume rendering and ray casting. They use the opacity and color of voxels to produce a 2D image by blending multiple voxels along each ray. Transfer functions control the opacity and color mapping, allowing for various rendering effects.</p>
</li>
<li>
<p><strong>Texture-Based Volume Rendering</strong>: This technique involves mapping a 2D texture onto a 3D object. It can be used to simulate the visualization of volume data, particularly in real-time applications.</p>
</li>
<li>
<p><strong>Slicing</strong>: Slicing involves cutting the volume into parallel or perpendicular slices and displaying these slices as 2D images. It is a straightforward way to visualize volumetric data, especially in the medical field.</p>
</li>
<li>
<p><strong>3D Texture Mapping</strong>: 3D textures store volumetric data and can be directly mapped onto 3D objects, allowing for interactive and real-time volume rendering.</p>
</li>
<li>
<p><strong>Shear-Warp Rendering</strong>: This method involves warping the volume data to align with the viewing plane, which simplifies the rendering process.</p>
</li>
<li>
<p><strong>Ray Marching</strong>: Ray marching is a technique where rays are cast through the volume data, and for each step along the ray, the accumulated information is used to generate the final image.</p>
</li>
<li>
<p><strong>Volume Ray Casting</strong>: Similar to ray marching, but it uses interpolation techniques to sample data values between voxels.</p>
</li>
<li>
<p><strong>Multi-Planar Reconstruction (MPR)</strong>: In medical imaging, MPR generates 2D images by reconstructing orthogonal slices through the 3D data, providing multi-angled views.</p>
</li>
<li>
<p><strong>Volume Clipping and Cropping</strong>: These techniques involve selectively rendering or visualizing a portion of the volume of interest.</p>
</li>
<li>
<p><strong>Volume Illumination</strong>: Techniques like shading and lighting can be applied to volume data for a more realistic appearance.</p>
</li>
<li>
<p><strong>Isosurface Extraction</strong>: This technique generates 3D surfaces representing specific intensity thresholds within the volume, which can then be visualized in 3D.</p>
</li>
<li>
<p><strong>Volume Rendering with Transfer Functions</strong>: Transfer functions are used to control the mapping of data values to color and opacity, allowing for enhanced visualization.</p>
</li>
</ol>
<p>These classic volume rendering techniques are often combined and customized to suit specific applications and achieve the desired visualization effects for various types of volumetric data. Advances in graphics hardware and algorithms have also led to the development of more sophisticated and real-time volume rendering methods.&quot;</p>
<h3 id="4-view-synthesis-a-name%22viewsynth%22a">4. View synthesis <a name="viewsynth"></a></h3>
<p>from ChatGPT, &quot;View synthesis, in the context of computer vision and computer graphics, refers to the process of generating new images or views of a scene or object from existing images or data. This technique is often used in various applications, including virtual reality, augmented reality, 3D modeling, and more. The goal is to create a coherent and realistic representation of a scene or object from different perspectives, even if those perspectives were not originally captured.</p>
<p>There are several methods for view synthesis, and they can vary depending on the specific application and available data. Some common approaches include:</p>
<ol>
<li>
<p><strong>Image-Based Rendering (IBR):</strong> This approach involves using a set of images captured from different viewpoints to generate new views. Techniques like image warping and texture mapping are often used to create the synthesized views.</p>
</li>
<li>
<p><strong>3D Reconstruction and Rendering:</strong> In this approach, a 3D model of the scene or object is constructed from the available data, such as depth maps, point clouds, or multi-view stereo data. Once the 3D model is created, new views can be rendered from different perspectives.</p>
</li>
<li>
<p><strong>Deep Learning:</strong> With the advancement of deep learning techniques, neural networks can be trained to synthesize views of a scene or object. For example, Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs) can be used for view synthesis tasks.</p>
</li>
</ol>
<p>View synthesis has numerous practical applications, such as creating 3D models from 2D images, simulating virtual environments, generating novel views of objects for video games or virtual reality, and enhancing the immersive experience in augmented reality applications. It is a fundamental concept in computer vision and computer graphics, and it continues to evolve with the development of new technologies and techniques.&quot;</p>
<iframe width="560" 
    height="315" 
    src="https://www.youtube.com/embed/1YVh0xPmuXg?si=WcVvXrdpH7RI1q-v" title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    allowfullscreen>
</iframe>
<h3 id="5-deep-fully-connected-neural-network-a-name%22fcnn%22a">5. Deep fully-connected neural network <a name="fcnn"></a></h3>
<p>from ChatGPT, &quot;A fully-connected, non-convolutional neural network, often referred to as a &quot;feedforward neural network&quot; or simply a &quot;multilayer perceptron (MLP),&quot; is a type of artificial neural network architecture used in machine learning and deep learning. Unlike convolutional neural networks (CNNs), which are specifically designed for tasks like image recognition, a fully-connected neural network is a more general architecture that can be used for various tasks, including image classification, natural language processing, and regression problems.</p>
<p>Here are some key characteristics of a fully-connected neural network:</p>
<ol>
<li>
<p><strong>Layer Structure</strong>: It consists of an input layer, one or more hidden layers, and an output layer. Each layer is fully connected to the next layer, which means that every neuron in one layer is connected to every neuron in the next layer.</p>
</li>
<li>
<p><strong>Neurons and Activation Functions</strong>: Neurons in each layer apply a linear transformation to the weighted sum of their inputs and then pass the result through an activation function. Common activation functions include sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU).</p>
</li>
<li>
<p><strong>Weights and Biases</strong>: Each connection between neurons is associated with a weight, and each neuron has a bias term. The network learns these weights and biases during training to make predictions.</p>
</li>
<li>
<p><strong>Feedforward Process</strong>: The information flows from the input layer through the hidden layers to the output layer in a feedforward manner. There are no loops or cycles in the network, which makes it a feedforward architecture.</p>
</li>
<li>
<p><strong>Loss Function and Backpropagation</strong>: Training the network involves adjusting the weights and biases to minimize a loss function that quantifies the difference between the predicted output and the actual target. Backpropagation is a technique used to update the network's parameters in the direction that reduces the loss.</p>
</li>
</ol>
<p>Fully-connected neural networks have been used in a wide range of applications, from image and speech recognition to natural language processing and financial forecasting. They can approximate complex functions but may not be as efficient as specialized architectures like CNNs for tasks involving structured data, such as images.</p>
<p>For tasks with grid-like data, like images, CNNs are more common due to their ability to learn spatial hierarchies and translation-invariant features. Fully-connected networks are typically used in the final layers of CNNs for tasks like image classification. However, in many other machine learning tasks, fully-connected networks can be effective, especially when combined with other techniques and architectures.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Tsvxx-GGlTg?si=oQCIWV3ItKy-Rhto" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="6-convolutional-layers-a-name%22conlay%22a">6. Convolutional layers <a name="conlay"></a></h3>
<p>from ChatGPT, &quot;Convolutional layers are a fundamental component of convolutional neural networks (CNNs), a class of deep learning models commonly used for tasks related to image and video analysis, as well as in other domains like natural language processing. Convolutional layers are designed to automatically and adaptively learn patterns, features, or representations directly from data, making them particularly effective for tasks where spatial relationships are important, such as image recognition.</p>
<p>Here's an overview of how convolutional layers work:</p>
<ol>
<li>
<p><strong>Convolution operation</strong>: Convolutional layers apply a convolution operation to their input data. This operation involves a small filter or kernel (a matrix of learnable parameters) sliding over the input data, computing the dot product between the filter and a local region of the input data at each position. This dot product represents the response of the filter to that specific region.</p>
</li>
<li>
<p><strong>Shared weights</strong>: One key characteristic of convolutional layers is weight sharing. The same filter is used at each position in the input data, allowing the layer to learn the same features regardless of their position. This reduces the number of parameters compared to fully connected layers and is especially useful for tasks like image recognition, where features are translation-invariant.</p>
</li>
<li>
<p><strong>Feature maps</strong>: As the filter slides over the input, it produces a feature map. A feature map is a 2D representation that highlights the presence of a specific feature or pattern in the input data.</p>
</li>
<li>
<p><strong>Multiple filters</strong>: Convolutional layers typically use multiple filters, each detecting different features. These filters learn to recognize various low-level and high-level features, such as edges, corners, textures, and even more complex structures as the layers go deeper in the network.</p>
</li>
<li>
<p><strong>Activation function</strong>: After the convolution operation, an activation function (e.g., ReLU - Rectified Linear Unit) is applied element-wise to the feature maps. This introduces non-linearity into the network, allowing it to learn complex relationships between features.</p>
</li>
<li>
<p><strong>Pooling</strong>: In between convolutional layers, pooling layers are often used to reduce the spatial dimensions of the feature maps. Max pooling, for example, takes the maximum value within a small region, reducing the size of the feature maps and providing some translation invariance.</p>
</li>
</ol>
<p>Convolutional layers are stacked in a CNN, with each layer capturing increasingly abstract and complex features. The final convolutional layers are often followed by one or more fully connected layers for making predictions or decisions based on the extracted features.</p>
<p>Overall, convolutional layers have revolutionized the field of computer vision and have been successfully applied to a wide range of tasks, including image classification, object detection, facial recognition, and more. They have also found applications beyond computer vision, such as in natural language processing for tasks like text classification and sequence modeling.&quot;</p>
<h3 id="7-convolutional-neural-networks-a-name%22cnn%22a">7. Convolutional neural networks <a name="cnn"></a></h3>
<p>from ChatGPT, &quot;A neural network is considered convolutional when it incorporates convolutional layers. Convolutional layers are a fundamental component of convolutional neural networks (CNNs) and are used for processing grid-like data, such as images and time series. What makes a neural network convolutional are the following key characteristics:</p>
<ol>
<li>
<p><strong>Convolutional Filters</strong>: Convolutional layers in a CNN use learnable convolutional filters (also called kernels). These filters are small grids of weights that are applied to local regions of the input data. The convolution operation involves element-wise multiplication of the filter with a local portion of the input data and then summing the results. This operation helps capture local patterns or features in the data.</p>
</li>
<li>
<p><strong>Spatial Hierarchies</strong>: Convolutional layers can learn to recognize hierarchical features in the data. The initial layers may capture low-level features like edges, corners, and textures, while deeper layers combine these low-level features to recognize more complex, high-level features and patterns.</p>
</li>
<li>
<p><strong>Weight Sharing</strong>: In traditional fully-connected neural networks, each connection between neurons has its own weight. In convolutional layers, the same filter is applied to different parts of the input data. This weight sharing reduces the number of parameters, making CNNs more efficient and capable of handling large input data like images.</p>
</li>
<li>
<p><strong>Local Connectivity</strong>: Convolutional layers have local connectivity, meaning each neuron in the layer is connected only to a small, localized region of the input data. This allows the network to focus on local patterns and is especially suitable for grid-like data where local patterns are important.</p>
</li>
<li>
<p><strong>Pooling Layers</strong>: CNNs often include pooling layers, which reduce the spatial dimensions of the data. Max-pooling and average-pooling are common operations in pooling layers, and they help downsample the feature maps created by convolutional layers, preserving the most important information.</p>
</li>
<li>
<p><strong>Translation Invariance</strong>: Convolutional layers have the property of translation invariance. This means that they can recognize patterns regardless of where they appear in the input. For example, a CNN can identify a specific feature in an image, regardless of its position.</p>
</li>
</ol>
<p>Convolutional neural networks are particularly well-suited for tasks like image classification, object detection, and image segmentation, where the spatial relationships in the data are important. They have been very successful in computer vision applications and are widely used in various domains where grid-like data is prevalent.</p>
<p>In contrast, fully-connected neural networks, also known as multilayer perceptrons (MLPs), lack the convolutional layers and are used for more general tasks where spatial relationships are not as crucial, such as text classification and regression.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/pj9-rr1wDhM?si=ARfgM30Ei0dgxH_X" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="8-multilayer-perceptron-a-name%22mlp%22a">8. Multilayer perceptron <a name="mlp"></a></h3>
<p>from ChatGPT, &quot;A Multilayer Perceptron (MLP) is a type of artificial neural network that consists of multiple layers of interconnected artificial neurons, also known as perceptrons. It is a feedforward neural network, which means that data flows in one direction, from the input layer through one or more hidden layers to the output layer.</p>
<p>Key characteristics of a Multilayer Perceptron include:</p>
<ol>
<li>
<p><strong>Input Layer</strong>: The input layer consists of neurons that receive the raw input data. Each neuron in the input layer represents a feature or attribute of the data.</p>
</li>
<li>
<p><strong>Hidden Layers</strong>: Between the input and output layers, one or more hidden layers are present. These layers are composed of interconnected neurons and are responsible for processing and learning complex patterns in the data.</p>
</li>
<li>
<p><strong>Neurons/Perceptrons</strong>: Each neuron in the network is connected to neurons in the adjacent layers and performs a weighted sum of its inputs, followed by the application of an activation function. The activation function introduces non-linearity into the network, allowing it to model complex relationships in the data.</p>
</li>
<li>
<p><strong>Weights and Biases</strong>: The connections between neurons are characterized by weights, which determine the strength of the connection, and biases, which introduce an offset. During training, the network learns optimal values for these weights and biases to minimize the error in its predictions.</p>
</li>
<li>
<p><strong>Activation Functions</strong>: Common activation functions used in MLPs include the sigmoid function, hyperbolic tangent (tanh), and rectified linear unit (ReLU). These functions introduce non-linearity and determine the output of each neuron.</p>
</li>
<li>
<p><strong>Output Layer</strong>: The output layer produces the final predictions or classifications. The number of neurons in the output layer depends on the specific problem; for instance, a binary classification problem would typically have one output neuron, while a multi-class classification problem might have multiple output neurons.</p>
</li>
<li>
<p><strong>Training</strong>: MLPs are trained using algorithms like backpropagation and gradient descent. During training, the network adjusts its weights and biases to minimize the difference between its predictions and the actual target values in the training data.</p>
</li>
</ol>
<p>Multilayer Perceptrons are a versatile class of neural networks that can be used for a wide range of machine learning tasks, including regression, classification, and function approximation. They are particularly useful for solving complex, non-linear problems and have been applied successfully in various domains, including image recognition, natural language processing, and more.&quot;</p>
<iframe width="560" 
    height="315" 
    src="https://www.youtube.com/embed/SNCpgqXddUs?si=cdiiJw-9gfsm59h2" title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    allowfullscreen>
</iframe>
<h3 id="9-marching-camera-rays-through-a-scene-a-name%22mcrtas%22a">9. Marching camera rays through a scene <a name="mcrtas"></a></h3>
<p>from ChatGPT, &quot;Marching camera rays through a scene is a technique commonly used in computer graphics, particularly in ray tracing and rendering. It involves simulating the path of light rays from a virtual camera through a 3D scene to generate a 2D image, mimicking the way real light behaves. Here's how the process works:</p>
<ol>
<li>
<p><strong>Camera Setup</strong>: The first step is to define the virtual camera's position, orientation, and other parameters, such as the field of view and image resolution. This camera represents the viewpoint from which the scene will be rendered.</p>
</li>
<li>
<p><strong>Ray Generation</strong>: For each pixel in the image, a ray is generated. The ray starts at the camera's position and travels through the pixel's location on the image plane. This forms the primary ray.</p>
</li>
<li>
<p><strong>Ray Traversal</strong>: The primary ray is extended into the 3D scene. As it travels through the scene, it may intersect with various objects, such as 3D models, surfaces, or volumes.</p>
</li>
<li>
<p><strong>Intersection Testing</strong>: At each point along the ray's path, an intersection test is performed to determine whether the ray hits an object in the scene. This involves checking if the ray's position is within the bounds of an object and, if so, calculating the point of intersection and other relevant information.</p>
</li>
<li>
<p><strong>Shading and Lighting</strong>: Once an intersection is found, the renderer calculates how the light interacts with the object at that point. This includes computing the lighting, reflection, refraction, and other optical properties of the materials and surfaces.</p>
</li>
<li>
<p><strong>Secondary Rays</strong>: In addition to primary rays, secondary rays may be generated in various situations. These include reflection rays (for reflective surfaces), refraction rays (for transparent materials), and shadow rays (to determine if a point is in shadow).</p>
</li>
<li>
<p><strong>Recursive Ray Tracing</strong>: The process can be recursive. For example, when a ray hits a reflective surface, a new reflected ray is generated and traced through the scene. This recursive process continues until a termination condition is met, such as a maximum recursion depth.</p>
</li>
<li>
<p><strong>Color Accumulation</strong>: As rays traverse the scene and interact with objects, the renderer accumulates color and lighting information along their paths. This information is combined to determine the final color of the pixel on the image plane.</p>
</li>
<li>
<p><strong>Image Formation</strong>: After processing all rays for all pixels, the final image is formed, representing the 3D scene from the camera's viewpoint. This image is then displayed or saved for further use.</p>
</li>
</ol>
<p>Marching camera rays through a scene is a fundamental technique in photorealistic rendering, as it allows for the simulation of complex lighting and optical effects, such as reflections, refractions, shadows, and global illumination. Ray tracing algorithms, like path tracing and ray marching, use this approach to create realistic images in computer graphics and visual effects.&quot;</p>
<iframe width="560" 
    height="315" 
    src="https://www.youtube.com/embed/BNZtUB7yhX4?si=NcNH0iBM-zFnbYJ2" title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    allowfullscreen>
</iframe>
<h3 id="10-gradient-descent-a-name%22gradesc%22a">10. Gradient descent <a name="gradesc"></a></h3>
<p>from ChatGPT, &quot;Gradient descent is an optimization algorithm used to minimize the cost or loss function in machine learning and deep learning models. It is a fundamental and widely used technique for adjusting the parameters of a model to find the values that result in the best possible performance.</p>
<p>The basic idea behind gradient descent is to iteratively update the model's parameters in the direction of steepest descent (i.e., the negative gradient of the cost function) in order to reach a minimum point of the cost function. Here's how the process works:</p>
<ol>
<li>
<p><strong>Initialization</strong>: Start with an initial set of parameters for the model. This can be random or based on some initial guess.</p>
</li>
<li>
<p><strong>Compute the Gradient</strong>: Calculate the gradient of the cost function with respect to each model parameter. The gradient represents the direction and magnitude of the steepest increase in the cost function.</p>
</li>
<li>
<p><strong>Update Parameters</strong>: Adjust the model parameters in the opposite direction of the gradient. This is done to reduce the cost function. The formula for updating the parameters typically looks like this:</p>
<p>New Parameter = Old Parameter - Learning Rate * Gradient</p>
<ul>
<li>The &quot;Learning Rate&quot; is a hyperparameter that controls the size of the steps taken during the optimization. It's a trade-off between convergence speed and the risk of overshooting the minimum.</li>
</ul>
</li>
<li>
<p><strong>Repeat</strong>: Steps 2 and 3 are repeated for a specified number of iterations or until a stopping criterion is met. The stopping criterion can be based on the change in the cost function or a fixed number of iterations.</p>
</li>
<li>
<p><strong>Convergence</strong>: Eventually, the algorithm converges to a local minimum or, in some cases, a global minimum of the cost function. The parameters at this minimum are considered the optimal values for the model.</p>
</li>
</ol>
<p>There are variations of gradient descent, including:</p>
<ul>
<li>
<p><strong>Stochastic Gradient Descent (SGD)</strong>: In this variant, the gradient is computed and the parameters are updated for a single training example at a time, rather than using the entire training dataset. This can lead to faster convergence and can be less computationally intensive.</p>
</li>
<li>
<p><strong>Mini-batch Gradient Descent</strong>: This combines the benefits of both gradient descent and SGD by updating the parameters using a small, random subset of the training data at each iteration.</p>
</li>
<li>
<p><strong>Batch Gradient Descent</strong>: The traditional form of gradient descent where the entire training dataset is used to compute the gradient and update the parameters at each iteration.</p>
</li>
</ul>
<p>The choice of gradient descent variant and the appropriate learning rate are important hyperparameters that can significantly impact the training of machine learning models. Proper tuning of these hyperparameters is often necessary to achieve optimal results.&quot;</p>
<iframe width="560" 
    height="315" 
    src="https://www.youtube.com/embed/qg4PchTECck?si=vnTb3e_d0GG8Ggbg" title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    allowfullscreen>
</iframe>
<h3 id="11-positional-encoding-a-name%22posenc%22a">11. Positional encoding <a name="posenc"></a></h3>
<p>from ChatGPT, &quot;Positional encoding is a technique used in the field of natural language processing (NLP) and specifically in transformer-based models, such as the original Transformer model and its variants (e.g., BERT, GPT). The purpose of positional encoding is to provide information about the positions or order of words in a sequence when dealing with sequences of data, such as text or time series data, in these models.</p>
<p>In transformers, input sequences are typically treated as sets, and the order of the elements is not inherently encoded because the models rely on self-attention mechanisms. However, for tasks that require knowledge of the position or order of elements, such as language understanding or generation, positional encoding is essential.</p>
<p>Positional encoding is usually added to the embeddings of the input tokens or elements. It consists of a set of learned positional embeddings that are combined with the token embeddings. These positional embeddings provide the model with information about the relative positions of elements within the sequence.</p>
<p>There are a few common methods for positional encoding:</p>
<ol>
<li>
<p><strong>Sine and Cosine Positional Encodings</strong>: This method was introduced in the original Transformer model. It uses a combination of sine and cosine functions with different frequencies to encode the position of each element in the sequence. The amplitude and frequency of the sine and cosine waves are learned during training.</p>
</li>
<li>
<p><strong>Learned Positional Encodings</strong>: In some models, such as BERT, the positional encodings are learned as part of the model's parameters. This means that the model can adaptively learn how to encode positional information along with other features.</p>
</li>
</ol>
<p>The choice of positional encoding method can depend on the specific architecture and task, but the primary goal is to provide the model with information about the sequence order. Without positional encoding, the model might struggle to differentiate between different positions in a sequence, which is critical for tasks that involve understanding the structure of language or making sequential predictions.</p>
<p>Positional encoding is just one component of the broader transformer architecture, which has proven highly effective in NLP and a wide range of other sequence-related tasks. It allows transformers to capture not only the content of input sequences but also their positions, making them versatile for various sequence processing tasks.&quot;</p>
<iframe width="560" 
    height="315" 
    src="https://www.youtube.com/embed/1biZfFLPRSY?si=t6xImnCXPhy_B94p" title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    allowfullscreen>
</iframe>
<h3 id="12-voxel-grids-a-name%22voxelgrids%22a">12. Voxel grids <a name="voxelgrids"></a></h3>
<p>from ChatGPT, &quot;Discretized voxel grids, often simply referred to as voxel grids, are three-dimensional grids used to represent and store volumetric data in a discrete and structured form. The term &quot;voxel&quot; is a portmanteau of &quot;volume&quot; and &quot;pixel,&quot; and it refers to the 3D equivalent of a 2D pixel. Voxel grids are commonly used in various fields, including computer graphics, computer-aided design, medical imaging, and scientific simulations.</p>
<p>Here are some key characteristics and uses of discretized voxel grids:</p>
<ol>
<li>
<p><strong>3D Representation</strong>: Voxel grids represent three-dimensional space as a regular grid of cubic or rectangular elements called voxels. Each voxel is a discrete unit that can store data or information about the space it occupies.</p>
</li>
<li>
<p><strong>Resolution</strong>: The resolution of a voxel grid is defined by the size of the voxels and the number of voxels along each dimension (length, width, and height). Higher resolution grids have smaller voxels and can capture finer details but require more memory.</p>
</li>
<li>
<p><strong>Data Representation</strong>: Voxel grids are used to represent various types of data, such as density values in volume rendering, color and opacity in 3D graphics, medical imaging data (e.g., MRI or CT scans), and physical properties in scientific simulations (e.g., fluid dynamics).</p>
</li>
<li>
<p><strong>Medical Imaging</strong>: Voxel grids are commonly used in medical imaging to represent the internal structures of the human body. Each voxel may store information about tissue density, allowing for the visualization of organs and anomalies in 3D.</p>
</li>
<li>
<p><strong>Computer Graphics</strong>: In 3D computer graphics, voxel grids can be used for volumetric rendering and modeling. They enable the creation of realistic volumetric effects like smoke, fire, and clouds. Voxel grids can also be used for constructive solid geometry (CSG) operations to create complex shapes.</p>
</li>
<li>
<p><strong>Scientific Simulations</strong>: Voxel grids are employed in scientific simulations, such as fluid dynamics and finite element analysis, where they represent physical properties within a 3D space. The simulation algorithms operate on the voxel data to model and analyze real-world phenomena.</p>
</li>
<li>
<p><strong>Molecular and Structural Biology</strong>: Voxel grids are used to represent molecular structures, such as proteins and DNA, in structural biology. They can be employed for tasks like docking simulations and electron density maps in X-ray crystallography.</p>
</li>
<li>
<p><strong>Augmented Reality and Virtual Reality</strong>: Voxel grids can be used in AR and VR applications to represent the 3D environment. This allows for realistic spatial interactions and visualizations.</p>
</li>
</ol>
<p>Voxel grids provide a structured and efficient way to work with 3D data, and they are particularly valuable when dealing with discrete and volumetric information. They are widely used in a variety of applications to represent, process, and visualize complex 3D data.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/dSDuR-45W6Y?si=fxlVN2zaG1w_g_5Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="12-signed-distance-functions-a-name%22sdf%22a">12. Signed distance functions <a name="sdf"></a></h3>
<p>from ChatGPT, &quot;A signed distance function (SDF) is a mathematical function that is commonly used in computer graphics, computer-aided design, computer vision, and physics simulations. It provides a way to represent geometric shapes, particularly in the context of 2D or 3D spaces, by assigning a signed distance value to each point in space. The key characteristics of signed distance functions are as follows:</p>
<ol>
<li>
<p><strong>Distance Information</strong>: The SDF assigns a real-valued distance to every point in space. This distance represents how far the point is from the nearest surface of the shape being described. The sign of the distance indicates whether the point is inside or outside the shape. Typically, points inside the shape have negative distances, points on the surface have a distance of zero, and points outside the shape have positive distances.</p>
</li>
<li>
<p><strong>Implicit Representation</strong>: SDFs provide an implicit representation of shapes, which means that you don't need to explicitly define the shape's boundaries or vertices. Instead, the shape is defined by the SDF itself, which makes it a versatile way to represent complex shapes without the need for a detailed mesh or explicit geometry.</p>
</li>
<li>
<p><strong>Efficiency</strong>: SDFs are often used in ray marching and ray tracing algorithms because they are efficient for ray-object intersection tests. They allow you to quickly determine how far a ray must travel to intersect a shape's surface.</p>
</li>
<li>
<p><strong>Boolean Operations</strong>: SDFs can be combined through mathematical operations like union, intersection, and subtraction to create complex shapes by manipulating the SDFs of simpler shapes. This is particularly useful in computer-aided design and computer graphics.</p>
</li>
<li>
<p><strong>Level Sets</strong>: SDFs are closely related to level set methods, where the zero-level set of an SDF represents the shape's boundary. Level set methods are used for shape evolution and tracking, such as in fluid simulations.</p>
</li>
<li>
<p><strong>Applications</strong>: SDFs find applications in a wide range of fields. In computer graphics, they are used for rendering, modeling, and collision detection. In computer vision, SDFs are used for object recognition and tracking. In physics simulations, they help with collision detection and response.</p>
</li>
</ol>
<p>Common geometric shapes, such as spheres, cubes, and tori, can be described using simple mathematical formulas as SDFs. More complex shapes can be represented through combinations of basic SDFs or by using procedural functions.</p>
<p>SDFs are a powerful tool for representing and manipulating geometry in a way that is versatile, efficient, and mathematically elegant. They have found a wide range of applications in computer science and engineering, especially in areas where complex shapes need to be represented or manipulated efficiently and accurately.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/62-pRVZuS5c?si=jWWuL4hQWZs8mGWo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="13-differentiable-rasterizers-a-name%22diffrast%22a">13. Differentiable rasterizers <a name="diffrast"></a></h3>
<p>from ChatGPT, &quot;Differentiable rasterizers are a class of algorithms and techniques used in computer graphics and deep learning that combine traditional rasterization (the process of converting 3D scene data into a 2D image) with differentiable operations. These differentiable rasterization techniques enable the end-to-end training of deep learning models by allowing gradients to flow through the rasterization process. This has applications in computer vision, 3D reconstruction, neural rendering, and other fields. Here are some key aspects of differentiable rasterizers:</p>
<ol>
<li>
<p><strong>Gradients through the Rasterization Process</strong>: In traditional rasterization, the rendering process is a discrete operation that does not naturally support backpropagation for gradient-based optimization. Differentiable rasterizers aim to bridge this gap by making the rendering process differentiable, meaning that gradients can be computed with respect to the model parameters and data inputs.</p>
</li>
<li>
<p><strong>Applications in Deep Learning</strong>: Differentiable rasterizers are used in conjunction with neural networks for tasks like 3D object detection, pose estimation, and shape reconstruction. They are particularly valuable when dealing with 3D data and neural networks that require end-to-end optimization.</p>
</li>
<li>
<p><strong>Differentiable Sampling</strong>: One approach to achieving differentiability is to use differentiable sampling techniques, such as bilinear interpolation or differentiable texture mapping, in the rasterization process. These methods allow the gradients of the image values to be computed with respect to changes in the underlying 3D scene data.</p>
</li>
<li>
<p><strong>Depth and Mask Prediction</strong>: In some differentiable rasterization techniques, the rasterizer predicts depth and mask information for each pixel. This allows the incorporation of spatial and depth information in the gradients, making it useful for tasks like depth estimation and 3D object pose recovery.</p>
</li>
<li>
<p><strong>Differentiable Rendering Loss</strong>: Differentiable rasterizers are often used in conjunction with differentiable rendering losses. These losses measure the difference between rendered images and target images, and they are used to train the network to produce more accurate renderings.</p>
</li>
<li>
<p><strong>Neural Rendering</strong>: Differentiable rasterizers are a key component of neural rendering approaches. These methods combine 3D data and neural networks to synthesize images that can be photorealistic and accurate. Neural rendering is used in virtual reality, augmented reality, and content creation.</p>
</li>
<li>
<p><strong>Differentiable Rasterization Libraries</strong>: There are libraries and frameworks that provide differentiable rasterization capabilities, making it easier for researchers and developers to incorporate these techniques into their deep learning pipelines.</p>
</li>
<li>
<p><strong>Challenges</strong>: While differentiable rasterization techniques have advanced the field of deep learning and computer graphics, they come with challenges related to numerical stability, scalability, and computational cost. Researchers are actively working on addressing these challenges.</p>
</li>
</ol>
<p>Differentiable rasterization has opened up new possibilities for deep learning applications in computer graphics and computer vision, enabling the training of models that can work directly with 3D data and generate images that are consistent with the underlying 3D scene. These techniques are a valuable tool for tasks that involve the interaction between 3D geometry and neural networks.&quot;</p>
<iframe width="560" 
    height="315" 
    src="https://www.youtube.com/embed/t7Ztio8cwqM?si=7ZrW7gQwfJWQr4Gl" title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    allowfullscreen>
</iframe>
<h3 id="13-volumetric-representation-a-name%22volrep%22a">13. Volumetric representation <a name="volrep"></a></h3>
<p>from ChatGPT, &quot;Volumetric representation in 3D scenes is a method for encoding and storing three-dimensional data in a way that describes the spatial distribution of a particular property or attribute throughout a volume of space. This representation is used in various fields, including computer graphics, medical imaging, scientific simulations, and more. Volumetric data is particularly well-suited for capturing complex and detailed spatial information.</p>
<p>Key aspects of volumetric representation in 3D scenes include:</p>
<ol>
<li>
<p><strong>Volume Elements (Voxels)</strong>: In volumetric representation, the 3D space is discretized into a grid of volume elements, which are referred to as &quot;voxels.&quot; Voxels are analogous to 2D pixels but exist in three dimensions. Each voxel corresponds to a small volume of space within the 3D scene.</p>
</li>
<li>
<p><strong>Attribute Information</strong>: For each voxel, one or more attributes or properties are stored. These attributes can represent a wide range of information, depending on the application. For example, in medical imaging, voxels can store information about tissue density or intensity, while in computer graphics, they may represent color, opacity, or other material properties.</p>
</li>
<li>
<p><strong>Density and Scalar Values</strong>: One common use of volumetric representation is to store scalar values at each voxel. These scalar values can represent physical quantities like temperature, pressure, or concentration, making volumetric data suitable for scientific simulations.</p>
</li>
<li>
<p><strong>Volumetric Rendering</strong>: Volumetric representation is often used for rendering 3D scenes, especially in the context of volumetric rendering. Volumetric rendering techniques use the information stored in voxels to generate 2D images that provide insights into the 3D volume. This is crucial in fields like medical imaging for visualizing internal structures.</p>
</li>
<li>
<p><strong>3D Reconstruction</strong>: Volumetric data is also used in 3D reconstruction from multiple 2D views or depth information. It allows for the creation of 3D models from a set of 2D images or depth maps.</p>
</li>
<li>
<p><strong>Medical Imaging</strong>: In medical imaging, volumetric data is essential for capturing detailed information about a patient's anatomy. CT (computed tomography) and MRI (magnetic resonance imaging) scans are common sources of volumetric data in the medical field.</p>
</li>
<li>
<p><strong>Scientific Simulations</strong>: Volumetric data is used to model physical phenomena in scientific simulations, such as fluid dynamics, weather modeling, and combustion simulations.</p>
</li>
<li>
<p><strong>Augmented and Virtual Reality</strong>: Volumetric data can be used in AR and VR applications to create immersive 3D environments. It provides rich spatial information for realistic interactions and visualizations.</p>
</li>
<li>
<p><strong>Discretization and Resolution</strong>: The quality of the volumetric representation depends on the resolution of the voxel grid. Higher resolutions capture finer details but require more memory and computational resources.</p>
</li>
</ol>
<p>Volumetric representation is a powerful and flexible way to capture 3D spatial data and is used across a wide range of applications where spatial information is crucial. It enables the visualization, analysis, and manipulation of complex 3D scenes and is an essential tool in fields that rely on spatial data.&quot;</p>
<h3 id="15-quadrature-a-name%22quadrature%22a">15. Quadrature <a name="quadrature"></a></h3>
<p>from ChatGPT, &quot;In mathematics, quadrature is a term that historically referred to various methods for calculating the area under a curve or the numerical approximation of definite integrals. The term &quot;quadrature&quot; is rarely used in modern mathematics and has largely been replaced with more specific and descriptive terminology, such as numerical integration or numerical methods for integration. Nevertheless, it's useful to understand the historical context and what quadrature originally referred to:</p>
<ol>
<li>
<p><strong>Numerical Integration</strong>: Quadrature methods were developed as techniques for approximating definite integrals. The definite integral of a function f(x) over a specific interval [a, b] represents the area under the curve of the function within that interval. Numerical integration methods, including quadrature, are used to approximate this area when it is not analytically solvable.</p>
</li>
<li>
<p><strong>Types of Quadrature</strong>: Quadrature methods encompass a variety of techniques for numerical integration, including the trapezoidal rule, Simpson's rule, and Gaussian quadrature. These methods differ in how they approximate the integral, with varying levels of accuracy and efficiency.</p>
</li>
<li>
<p><strong>Historical Significance</strong>: Quadrature was a significant problem in mathematics, particularly in ancient Greece and during the time of the ancient mathematician Archimedes. Archimedes used a geometric approach to approximate the value of π by finding the area of a circle, which can be thought of as a quadrature problem. The term &quot;quadrature&quot; has historical ties to solving problems related to the measurement of areas and volumes.</p>
</li>
<li>
<p><strong>Use in Numerical Analysis</strong>: Even though the term &quot;quadrature&quot; is less commonly used today, the methods and principles behind quadrature remain important in numerical analysis. Numerical integration techniques are widely used for approximating integrals in scientific and engineering applications, including physics simulations, data analysis, and more.</p>
</li>
</ol>
<p>In summary, &quot;quadrature&quot; historically referred to techniques for approximating definite integrals, and it is a precursor to modern numerical integration methods. While the term itself is not as prevalent in contemporary mathematics, the principles and methods of quadrature are fundamental to numerical analysis and have numerous practical applications.&quot;</p>
<h3 id="16-stratified-sampling-a-name%22sampling%22a">16. Stratified sampling <a name="sampling"></a></h3>
<p>from ChatGPT, &quot;Stratified sampling is a statistical sampling technique used in research and data analysis to ensure that a sample drawn from a population is representative of the various subgroups or strata within that population. This method is particularly useful when the population can be divided into distinct groups or categories, and researchers want to make sure that each subgroup is adequately represented in the sample.</p>
<p>Here's how stratified sampling works:</p>
<ol>
<li>
<p><strong>Dividing the Population into Strata</strong>: The first step is to divide the population into mutually exclusive and exhaustive strata or groups. These strata are typically defined based on certain characteristics or attributes, such as age, gender, income, geographic location, or any other relevant factors.</p>
</li>
<li>
<p><strong>Random Sampling within Each Stratum</strong>: Within each stratum, random sampling is performed. This means that a random sample is drawn from each subgroup. The sample size from each stratum is often proportional to the size of that stratum relative to the entire population. In other words, larger strata contribute more samples, while smaller strata contribute fewer samples.</p>
</li>
<li>
<p><strong>Combining the Samples</strong>: Once samples are collected from each stratum, they are combined to create the overall sample for analysis. The resulting sample is a stratified sample.</p>
</li>
</ol>
<p>The main advantages of stratified sampling are:</p>
<ol>
<li>
<p><strong>Improved Representativeness</strong>: Stratified sampling ensures that each subgroup of the population is represented in the sample, which makes the sample more representative of the entire population. This can lead to more accurate and reliable results.</p>
</li>
<li>
<p><strong>Greater Precision</strong>: By focusing on each stratum individually and appropriately allocating sample sizes, researchers can achieve greater precision and reduce sampling error.</p>
</li>
<li>
<p><strong>Comparisons between Strata</strong>: Researchers can compare and analyze different subgroups within the population with more confidence, as they know that each subgroup has been adequately sampled.</p>
</li>
</ol>
<p>Stratified sampling is commonly used in various fields, including market research, political polling, educational research, and scientific studies, where researchers want to ensure that their sample accurately reflects the diversity or stratification within the population of interest.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/sYRUYJYOpG0?si=lF-L1-e4grj9Z2rm" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="17-alpha-compositing-porter-duff-model-a-name%22pdc%22a">17. Alpha compositing (Porter-Duff) model <a name="pdc"></a></h3>
<p>from ChatGPT, &quot;Traditional alpha compositing, often referred to as the Porter-Duff compositing model, is a method used in computer graphics and image processing to combine multiple layers or images with transparency (alpha) information to create a single composited image. This method defines how the color and transparency of each pixel in a layer or image affect the final result when combined with other layers.</p>
<p>The Porter-Duff compositing model was introduced by Thomas Porter and Tom Duff in 1984 and has since become a fundamental concept in computer graphics. It defines a set of basic compositing operations based on the alpha value (transparency) of each pixel. The model provides a way to express how to blend and combine pixels in a visually intuitive manner.</p>
<p>The basic Porter-Duff compositing operations include:</p>
<ol>
<li>
<p><strong>Over</strong>: This operation combines two images, one representing the source and the other the background. It respects the alpha (transparency) value of each pixel in the source image, blending it with the background image. The result is an image that appears on top of the background, respecting transparency.</p>
</li>
<li>
<p><strong>In</strong>: This operation takes the intersection of the source and background images, considering the alpha values. It results in an image where the source appears within the boundaries defined by the background's alpha values.</p>
</li>
<li>
<p><strong>Out</strong>: This operation is the opposite of &quot;In.&quot; It results in an image where the source appears outside the boundaries defined by the background's alpha values.</p>
</li>
<li>
<p><strong>Atop</strong>: This operation combines the source and background, but the source is clipped to the alpha values of the background. The result is that the source is blended with the background, taking on the alpha values of the background.</p>
</li>
<li>
<p><strong>XOR</strong>: This operation combines the source and background, but the regions where they overlap are removed, creating a distinctive visual effect.</p>
</li>
<li>
<p><strong>Plus</strong>: This operation adds the color values of the source and the background, taking into account their alpha values.</p>
</li>
</ol>
<p>These basic compositing operations can be used to create a wide range of visual effects by controlling how layers with transparency are blended together. The Porter-Duff model provides a standardized way of specifying how these operations are applied to create complex composites in computer graphics and image editing software.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/atR7Mam1Yds?si=XiU5X8uBcjSatJOs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="18-adam-optimizier-a-name%22adam%22a">18. Adam optimizier <a name="adam"></a></h3>
<p>from ChatGPT, &quot;The Adam optimizer, short for Adaptive Moment Estimation, is a popular optimization algorithm used in training machine learning models, particularly in the context of deep learning and neural networks. It combines the advantages of two other optimization techniques: RMSprop (Root Mean Square Propagation) and momentum. Adam is known for its efficiency, fast convergence, and robustness across a wide range of deep learning tasks.</p>
<p>Here are the key features and components of the Adam optimizer:</p>
<ol>
<li>
<p><strong>Adaptive Learning Rates</strong>: Adam adjusts the learning rates dynamically for each parameter during training. It uses the moving averages of past gradients to compute a per-parameter learning rate. This adaptive learning rate helps improve the convergence of the optimization process and can prevent the model from getting stuck in local minima.</p>
</li>
<li>
<p><strong>Momentum</strong>: Adam includes a momentum term that helps accelerate convergence, especially when the gradient direction remains consistent. The momentum term, similar to the momentum optimizer, accumulates past gradients to give the optimization process an additional &quot;push.&quot;</p>
</li>
<li>
<p><strong>Root Mean Square Propagation (RMSprop)</strong>: Adam uses an element-wise square root of the exponentially moving average of past squared gradients to normalize the updates for each parameter. This helps control the learning rate for each parameter separately.</p>
</li>
<li>
<p><strong>Bias Correction</strong>: To address bias issues in the early iterations of training, Adam applies bias correction to the moving averages of the gradients. This correction helps in stabilizing the optimization process at the beginning of training.</p>
</li>
</ol>
<p>The update step in Adam for each parameter θ is calculated as follows:</p>
<ol>
<li>
<p>Compute the current gradient g_t with respect to the loss function.</p>
</li>
<li>
<p>Update the first moment estimate m_t and the second moment estimate v_t using exponential moving averages:</p>
</li>
</ol>
<p>$$m_t = β1 * m_{t-1} + (1 - β_1) * g_t$$
$$v_t = β2 * v_{t-1} + (1 - β_2) * (g_t^2)$$</p>
<p>Here, $β_1$ and $β_2$ are hyperparameters (typically close to 1) controlling the exponential decay rates.</p>
<ol start="3">
<li>
<p>Apply bias correction to the first and second moment estimates to mitigate bias issues:</p>
<p>$$m_t^{corrected} = m_t / (1 - β1^t)$$
$$v_t^{corrected} = v_t / (1 - β2^t)$$</p>
</li>
<li>
<p>Compute the parameter update:</p>
<p>$$Δθ_t = -α * m_t^{corrected} / (sqrt(v_t^{corrected}) + ε)$$</p>
<p>Here, α is the learning rate, and ε is a small constant added to prevent division by zero.</p>
</li>
<li>
<p>Update the model parameters:</p>
<p>$$θ_{t+1} = θ_t + Δθ_t$$</p>
</li>
</ol>
<p>Adam is widely used because it tends to work well &quot;out of the box&quot; for a variety of deep learning tasks. However, the choice of hyperparameters, such as the learning rate and the β values, can still significantly impact the performance of the optimizer. Researchers have also proposed variations and improvements on Adam, such as AdaGrad, RMSprop, and Nadam, which have their own advantages in specific situations.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/JXQT_vxqwIs?si=5DpIZZS3YyZuJ5U1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="18-lambertian-objects-a-name%22lamb%22a">18. Lambertian objects <a name="lamb"></a></h3>
<p>from ChatGPT, &quot;Lambertian objects, in the context of computer graphics and computer vision, are objects that exhibit a specific type of diffuse reflection behavior called Lambertian reflectance. Lambertian reflectance is an idealized model used to describe how light interacts with a surface when it scatters uniformly in all directions. Objects that follow Lambertian reflectance have certain distinctive characteristics:</p>
<ol>
<li>
<p><strong>Diffuse Reflection</strong>: Lambertian objects exhibit purely diffuse reflection, meaning that incoming light is scattered uniformly in all directions. This is in contrast to specular reflection, where light is reflected in a focused manner, like a mirror.</p>
</li>
<li>
<p><strong>Constant Brightness</strong>: The brightness of Lambertian objects appears constant regardless of the viewing angle. This means that the color and intensity of the object's surface do not change when viewed from different angles.</p>
</li>
<li>
<p><strong>Independence of Light Source</strong>: Lambertian reflectance is independent of the light source direction. The object appears equally bright when illuminated from any direction, as long as it is illuminated by the same total amount of light.</p>
</li>
<li>
<p><strong>No Glossiness or Shininess</strong>: Unlike materials with specular highlights, Lambertian objects lack glossiness or shininess. They have a matte or flat appearance without distinct highlights or reflections.</p>
</li>
</ol>
<p>Lambertian reflectance is described by Lambert's cosine law, which states that the reflected radiance (brightness) from a Lambertian surface is directly proportional to the cosine of the angle between the surface normal (a vector perpendicular to the surface) and the incoming light direction. This relationship can be expressed as:</p>
<p>$$L_o(v) = \rho \cdot \frac{\mathbf{n} \cdot \mathbf{l}}{\pi}$$</p>
<p>Where:</p>
<ul>
<li>$L_o(v)$ is the outgoing radiance in the view direction (\mathbf{v}).</li>
<li>$\rho$ is the surface reflectance (albedo), indicating how much of the incident light is reflected.</li>
<li>$\mathbf{n}$ is the normalized surface normal vector.</li>
<li>$\mathbf{l}$ is the normalized incoming light direction.</li>
</ul>
<p>Lambertian reflectance is a simple and widely used model in computer graphics and computer vision for various purposes, including rendering, lighting calculations, and estimating surface normals from shading. While many real-world surfaces do not perfectly follow Lambertian reflectance due to factors like specularity and surface properties, it serves as a foundational model for understanding how light interacts with surfaces and is used in various rendering and shading algorithms.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/HPNW0we-ft0?si=t4vcMSnAkupQ3yoR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="19-peak-signal-to-noise-ration-psnr-a-name%22psnr%22a">19. Peak signal-to-noise ration (PSNR) <a name="psnr"></a></h3>
<p>from ChatGPT, &quot;PSNR stands for &quot;Peak Signal-to-Noise Ratio.&quot; It is a metric used to measure the quality of reconstructed or compressed data, particularly in the context of digital image or video processing. PSNR is expressed in decibels (dB) and is used to quantify the amount of noise or distortion present in a signal when compared to the original, uncompressed signal.</p>
<p>The formula for calculating PSNR is as follows:</p>
<p>PSNR = 10 * log10((MAX^2) / MSE)</p>
<p>Where:</p>
<ul>
<li>PSNR is the Peak Signal-to-Noise Ratio in decibels.</li>
<li>MAX is the maximum possible pixel value (e.g., 255 for 8-bit images).</li>
<li>MSE is the Mean Squared Error, which is the average of the squared differences between corresponding pixels in the original and reconstructed images.</li>
</ul>
<p>A higher PSNR value indicates a higher quality image or video, as it means that the signal is closer to the original and contains less noise or distortion. Conversely, a lower PSNR value suggests a lower quality result with more noticeable artifacts.</p>
<p>PSNR is widely used in image and video compression, transmission, and processing to assess the fidelity of the processed or compressed data. It provides a quantitative measure of how well the processed signal retains the information present in the original, making it a valuable tool for quality assessment in various multimedia applications.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/mGPJBNsq7bA?si=VEqY4DHyN4LuE9lz" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="20-structural-similarity-index-ssim-a-name%22ssim%22a">20. Structural similarity index (SSIM) <a name="ssim"></a></h3>
<p>from ChatGPT, &quot;SSIM stands for &quot;Structural Similarity Index.&quot; It is a widely used metric for assessing the structural similarity between two images. SSIM is designed to measure the perceived quality of an image by comparing the structural information in the original image to a distorted or compressed version of that image. It's particularly valuable for evaluating the visual quality of images after various image processing tasks, including compression, denoising, and resizing.</p>
<p>The SSIM index is a decimal value that ranges from -1 to 1, with 1 indicating a perfect similarity between the two images. The SSIM formula takes into account three components: luminance, contrast, and structure.</p>
<p>The formula for calculating SSIM is relatively complex, but it can be summarized as:</p>
<p>SSIM(x, y) = (L(x, y) * C(x, y) * S(x, y))</p>
<p>Where:</p>
<ul>
<li>L(x, y) represents the luminance comparison, which measures the similarity in brightness or intensity between the two images.</li>
<li>C(x, y) represents the contrast comparison, which assesses the similarity in contrast between the two images.</li>
<li>S(x, y) represents the structure comparison, which examines the structural similarity between the two images.</li>
</ul>
<p>The product of these three components gives the overall SSIM index.</p>
<p>A higher SSIM score indicates a higher degree of similarity and, therefore, better image quality. Conversely, a lower SSIM score indicates greater dissimilarity and a lower quality image.</p>
<p>SSIM has become a standard method for evaluating image quality, and it is often preferred over other metrics like PSNR (Peak Signal-to-Noise Ratio) because it takes into account structural information and better aligns with human perception of image quality. Researchers and professionals in fields such as image and video processing, computer vision, and quality assessment frequently use SSIM to gauge the quality of their image processing algorithms and systems.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/699WYEx1qEg?si=BynbxOI7sh_4TKxu" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="21-learned-perceptual-image-patch-similarity-lpips-a-name%22lpips%22a">21. Learned perceptual image patch similarity (LPIPS) <a name="lpips"></a></h3>
<p>from ChatGPT, &quot;LPIPS stands for &quot;Learned Perceptual Image Patch Similarity,&quot; and it is a metric used for assessing the perceptual similarity between two images. LPIPS is designed to capture the perceptual differences between images as perceived by humans, and it takes into account the visual features that are relevant to human perception.</p>
<p>LPIPS is particularly useful in the field of computer vision and image processing for tasks such as image quality assessment, image generation, and image-to-image translation. Unlike traditional image quality metrics like PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index), LPIPS is based on deep neural networks and is learned from a large dataset of images. This makes it capable of capturing more complex and high-level perceptual differences between images.</p>
<p>LPIPS operates by comparing image patches and their respective features extracted by a neural network. The differences in these features are used to calculate a similarity score. The final LPIPS score is an aggregate of the similarity scores computed over various image patches.</p>
<p>LPIPS has gained popularity because it aligns more closely with human perception and can provide better assessments of image quality and similarity in tasks such as image super-resolution, style transfer, and image-to-image translation.</p>
<p>The specific architecture and training of LPIPS models may vary, and there are different versions of LPIPS with improvements in accuracy and computational efficiency. Researchers and practitioners in computer vision and image processing often use LPIPS to evaluate and compare the quality and perceptual fidelity of various image processing and generation algorithms.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/eMh9AcI77gg?si=7QhWcoNM9Bw4g-XO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="22-diffuse-objects-a-name%22diffobj%22a">22. Diffuse objects <a name="diffobj"></a></h3>
<p>from ChatGPT, &quot;In the context of computer graphics, computer vision, and 3D rendering, &quot;diffuse objects&quot; refer to surfaces or materials that exhibit diffuse reflection. Diffuse reflection is a type of reflection where light is scattered uniformly in all directions when it interacts with the surface, as opposed to being reflected at a specific angle as in specular reflection.</p>
<p>Key characteristics of diffuse objects include:</p>
<ol>
<li>
<p>Uniform Reflection: Diffuse objects appear equally bright from all viewing angles because light is scattered in all directions. This creates a soft and even appearance with no shiny spots or highlights.</p>
</li>
<li>
<p>Lambertian Reflectance: The reflectance model commonly used to describe the behavior of diffuse objects is Lambertian reflectance. According to Lambert's law, the brightness of the surface is proportional to the cosine of the angle between the surface normal and the incoming light direction.</p>
</li>
<li>
<p>Lack of Specular Highlights: Diffuse objects do not produce distinct specular highlights, which are the bright, concentrated reflections you might see on shiny or specular surfaces like metals or glass.</p>
</li>
</ol>
<p>Common examples of diffuse objects in the real world include materials like paper, fabric, unpolished wood, and some matte paints. These materials are said to have a diffuse or matte appearance because they scatter light evenly in all directions, resulting in a soft, non-reflective surface.</p>
<p>In computer graphics and 3D rendering, simulating the appearance of diffuse objects is essential for creating realistic and natural-looking scenes. Understanding how light interacts with diffuse surfaces is a fundamental concept in rendering and shading, and it plays a crucial role in determining the overall visual quality of computer-generated images and animations.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/EHn4asdgAos?si=KMLNP2g1_15bOteg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="23-forward-facing-captures-a-name%22forward%22a">23. Forward facing captures <a name="forward"></a></h3>
<p>from ChatGPT, &quot;The term &quot;forward-facing captures&quot; typically refers to images or data that are captured or recorded by a device, such as a camera, sensor, or scanner, in a direction that is oriented towards the front or forward direction of the device. These captures are taken from the perspective of what the device is directly facing.</p>
<p>In various contexts, forward-facing captures can be important and serve different purposes:</p>
<ol>
<li>
<p><strong>Cameras and Photography:</strong> In the context of cameras, forward-facing captures are images taken by the camera's primary lens or sensor, looking straight ahead. These images are what a person or a device in front of the camera would see.</p>
</li>
<li>
<p><strong>Autonomous Vehicles:</strong> In the field of autonomous vehicles, forward-facing cameras and sensors are essential for providing real-time data about the road and traffic conditions ahead. These captures help with tasks like lane-keeping, object detection, and collision avoidance.</p>
</li>
<li>
<p><strong>Virtual Reality (VR) and Augmented Reality (AR):</strong> In VR and AR applications, forward-facing captures are used to provide the user with a view that simulates the real-world perspective. This is especially important for creating immersive experiences where the user feels like they are in the environment.</p>
</li>
<li>
<p><strong>Surveillance and Security:</strong> Forward-facing cameras are often used in surveillance and security systems to monitor and record activities in a specific area or direction.</p>
</li>
<li>
<p><strong>Navigation:</strong> GPS and mapping systems in smartphones and other devices often use forward-facing captures to help users with turn-by-turn navigation and provide real-time images of the user's current location.</p>
</li>
</ol>
<p>In the context of different devices and applications, the term &quot;forward-facing captures&quot; is used to describe the data or imagery collected from the viewpoint directly in front of the device. This information is valuable for a wide range of applications, from photography to autonomous systems, navigation, and immersive technologies.&quot;</p>
<h3 id="24-scene-representation-networks-srn-a-name%22srn%22a">24. Scene representation networks (SRN) <a name="srn"></a></h3>
<p>from ChatGPT, &quot;Scene Representation Networks (SRNs) are a class of neural network models used in computer vision and 3D scene understanding. These networks are designed to capture and represent complex 3D scenes, including their geometry and appearance, in a way that allows for various downstream tasks, such as object recognition, scene reconstruction, and view synthesis.</p>
<p>Key characteristics and capabilities of Scene Representation Networks include:</p>
<ol>
<li>
<p><strong>3D Scene Understanding:</strong> SRNs are focused on understanding and representing 3D scenes. They can model the geometry of the scene, including the shapes of objects and their positions in 3D space.</p>
</li>
<li>
<p><strong>View Synthesis:</strong> SRNs can synthesize novel views of a scene from different angles or viewpoints. This is particularly useful in applications like 3D scene reconstruction and virtual reality.</p>
</li>
<li>
<p><strong>Multi-Modal Data:</strong> SRNs can handle multi-modal data, including 2D images and 3D data, to create comprehensive scene representations. This makes them versatile for tasks that involve different types of sensory information.</p>
</li>
<li>
<p><strong>Neural Rendering:</strong> They often incorporate techniques from neural rendering, which is a field that leverages neural networks to generate images or views of scenes that may not exist in their entirety in the training data. This can involve techniques like novel view synthesis or texture mapping.</p>
</li>
<li>
<p><strong>Application Areas:</strong> SRNs are used in a variety of applications, including robotics, augmented reality, virtual reality, and autonomous navigation, where understanding and representing 3D scenes are essential.</p>
</li>
</ol>
<p>The specific architecture and methodology of SRNs can vary depending on the application and the research or development work involved. Researchers in computer vision and 3D scene understanding continue to explore and develop SRN models to advance the state-of-the-art in 3D scene representation, rendering, and related fields.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/wbbfK06xzbw?si=4DObyaI48OkcEkNo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="25-local-light-field-fusion-llff-a-name%22llff%22a">25. Local Light Field Fusion (LLFF) <a name="llff"></a></h3>
<p>from ChatGPT, &quot;Local Light Field Fusion is a computational photography technique used to enhance the quality of images captured in a light field photography setup. Light field photography involves capturing both the intensity and direction of light rays at every point in space within a scene. This rich information allows for various post-capture manipulations and effects, including refocusing, depth estimation, and the creation of synthetic views.</p>
<p>Local Light Field Fusion specifically focuses on the process of combining or fusing the captured information from a light field into a single image, which may involve various improvements or optimizations. This fusion process aims to produce a high-quality image that can be used for various purposes, such as creating a depth-of-field effect, enhancing image resolution, or generating a standard 2D image from the light field data.</p>
<p>The fusion can involve several steps and considerations:</p>
<ol>
<li>
<p><strong>Depth-of-Field Control:</strong> One common use of Local Light Field Fusion is to simulate depth-of-field effects. By selectively combining information from different rays within the light field, the photographer can control what is in focus and what is blurred in the final image.</p>
</li>
<li>
<p><strong>Resolution Enhancement:</strong> Combining information from multiple rays can lead to higher-resolution output images. This can be beneficial in creating images with more detail and sharpness.</p>
</li>
<li>
<p><strong>Noise Reduction:</strong> Light field data can be noisy due to the multiple perspectives captured. Fusion techniques can help reduce noise and improve the overall image quality.</p>
</li>
<li>
<p><strong>Artifact Removal:</strong> In the process of combining the light field data, certain artifacts, such as aliasing or moiré patterns, may be introduced. Local Light Field Fusion can be used to mitigate or remove these artifacts.</p>
</li>
<li>
<p><strong>Synthetic View Generation:</strong> The fusion process can also be used to generate novel views or perspectives of the scene, which can be valuable in applications like virtual reality or 3D modeling.</p>
</li>
</ol>
<p>The exact methods and algorithms used for Local Light Field Fusion can vary depending on the specific goals of the fusion process and the available computational resources. Researchers and photographers continue to explore and develop techniques for harnessing the potential of light field photography and improving the quality and creative possibilities of the resulting images.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/LY6MgDUzS3M?si=biQ0vfNr_aJRayx0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="26-large-3d-voxel-grids-a-name%22l3vg%22a">26. Large 3D voxel grids <a name="l3vg"></a></h3>
<p>from ChatGPT, &quot;A large 3D voxel grid is a data structure used to represent and store three-dimensional information in a discrete, grid-like format. Voxel stands for &quot;volume element,&quot; and these grids are composed of small, regularly spaced cube-shaped units, similar to the pixels in a 2D image. However, in a 3D voxel grid, these units extend into three dimensions, forming a cube-like lattice in 3D space.</p>
<p>Each individual cube in a 3D voxel grid is called a voxel and represents a small volume element within the 3D space. Voxel grids are commonly used in various fields, including computer graphics, computer-aided design, medical imaging, and scientific simulations. Here are some key features and applications of large 3D voxel grids:</p>
<ol>
<li>
<p><strong>Representation of 3D Space:</strong> Voxel grids are used to represent objects, scenes, or structures in three dimensions. Each voxel can store various types of information, such as color, material properties, or density, depending on the application.</p>
</li>
<li>
<p><strong>Volume Rendering:</strong> In medical imaging and scientific visualization, large 3D voxel grids are used to represent volumetric data, such as CT scans or MRI images. This allows for rendering and analysis of 3D structures and internal features.</p>
</li>
<li>
<p><strong>Computer Graphics:</strong> In 3D computer graphics, voxel grids are used for representing objects, environments, or terrain. They can be used for techniques like voxel-based rendering and modeling.</p>
</li>
<li>
<p><strong>Simulation:</strong> Voxel grids are employed in physics simulations, such as fluid dynamics and finite element analysis, where they represent the 3D space for simulating physical phenomena.</p>
</li>
<li>
<p><strong>Game Development:</strong> Some video games use voxel grids to create destructible environments or block-based worlds, such as in voxel-based sandbox games like Minecraft.</p>
</li>
</ol>
<p>The size of a large 3D voxel grid can vary significantly, depending on the requirements of the specific application. Large grids may encompass vast 3D spaces, such as entire virtual worlds or detailed medical imaging volumes.</p>
<p>Voxel grids provide a discrete representation of 3D space, allowing for efficient storage and processing of 3D data, and they are a fundamental data structure in fields that deal with 3D information and spatial analysis.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/bfLFcBk1sBI?si=TgDAlqx9C5A4VeRg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="27-relu-activations-relu--activation-functions-a-name%22relu%22a">27. ReLU activations (ReLU) / Activation functions <a name="relu"></a></h3>
<p>from ChatGPT, &quot;In neural networks, an activation function is a mathematical function that determines the output of a neuron, which is a basic processing unit in the network. Neurons take in input values, perform computations, and produce an output that gets passed to the next layer of neurons. Activation functions introduce non-linearity to the network, allowing it to learn complex patterns and relationships in the data. They are crucial for the network to model real-world problems effectively.</p>
<p>There are several types of activation functions used in neural networks, and they can be categorized into linear and nonlinear functions. Here are some common types:</p>
<ol>
<li>
<p><strong>Linear Activation Function (Identity Function):</strong> This is the simplest activation function. It computes the output as a linear combination of the input, meaning it doesn't introduce any non-linearity. The output is proportional to the input. However, this type of activation is rarely used in hidden layers because it makes the entire network behave like a linear model.</p>
<p>Linear Activation: f(x) = x</p>
</li>
<li>
<p><strong>Nonlinear Activation Functions:</strong></p>
<p>a. <strong>Sigmoid Activation (Logistic Activation):</strong> The sigmoid function maps input values to a range between 0 and 1, which can be interpreted as a probability. It's useful in the output layer of binary classification models.</p>
<p>Sigmoid Activation: f(x) = 1 / (1 + exp(-x))</p>
<p>b. <strong>Hyperbolic Tangent (Tanh) Activation:</strong> The tanh function maps input values to a range between -1 and 1. It is also used for hidden layers in neural networks.</p>
<p>Tanh Activation: f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))</p>
<p>c. <strong>Rectified Linear Unit (ReLU) Activation:</strong> ReLU is a popular nonlinear activation function. It returns the input value for positive inputs and outputs zero for negative inputs. This introduces non-linearity while being computationally efficient.</p>
<p>ReLU Activation: f(x) = max(0, x)</p>
</li>
</ol>
<p>These activation functions play a crucial role in the learning process of neural networks. They enable the network to approximate complex functions, learn meaningful features from the data, and make predictions or classifications.</p>
<p>Now, focusing on ReLU (Rectified Linear Unit) activation, as mentioned earlier, it's a widely used activation function in neural networks due to its non-linearity and effectiveness in addressing vanishing gradient problems. It introduces non-linearity by outputting the input value for positive inputs and zero for negative inputs, making it computationally efficient and well-suited for training deep neural networks. This non-linearity helps the network learn and model complex patterns and relationships within the data.</p>
<p>ReLU, which stands for Rectified Linear Unit, is an activation function commonly used in artificial neural networks and deep learning models. It's a simple, yet highly effective, nonlinear activation function that introduces non-linearity to the model. ReLU activation is particularly popular in various neural network architectures due to its ability to address the vanishing gradient problem, which can hinder the training of deep networks.</p>
<p>The ReLU activation function is defined as follows:</p>
<p>ReLU(x) = max(0, x)</p>
<p>In this function:</p>
<ul>
<li>If the input value (x) is positive, ReLU returns the input value as-is.</li>
<li>If the input value is negative, ReLU returns 0.</li>
</ul>
<p>Mathematically, this can be expressed as:</p>
<p>ReLU(x) = {
x, if x &gt; 0
0, if x &lt;= 0
}</p>
<p>The key characteristics of ReLU activation include:</p>
<ol>
<li>
<p><strong>Non-linearity:</strong> ReLU introduces non-linearity to the network, allowing it to model and approximate complex, nonlinear relationships in the data.</p>
</li>
<li>
<p><strong>Sparsity:</strong> ReLU activations are sparse, meaning that they activate only for positive values. This can lead to more efficient network training and reduced computational requirements.</p>
</li>
<li>
<p><strong>Vanishing Gradient Mitigation:</strong> Unlike some other activation functions like sigmoid or tanh, ReLU does not saturate for positive input values, mitigating the vanishing gradient problem and allowing for the training of deep neural networks.</p>
</li>
</ol>
<p>Despite its advantages, ReLU can also exhibit a problem called the &quot;dying ReLU&quot; problem. In some cases, neurons with ReLU activation can become inactive and output only zeros for all inputs, making them untrainable. Researchers have introduced variations of ReLU, such as Leaky ReLU, Parametric ReLU (PReLU), and Exponential Linear Unit (ELU), to address these issues and further improve the training and performance of neural networks. These variants aim to provide non-zero gradients for negative input values, ensuring that neurons continue to learn even during training.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/68BZ5f7P94E?si=12FwZQe3QcAc4iWI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="28-sigmoid-activatoin-a-name%22sigmoid%22a">28. Sigmoid activatoin <a name="sigmoid"></a></h3>
<p>from ChatGPT, &quot;Sigmoid activation, often referred to as the sigmoid function or logistic activation, is a type of nonlinear activation function commonly used in artificial neural networks. It is specifically used to introduce non-linearity in the output of a neuron or node in a neural network. The sigmoid function maps input values to a range between 0 and 1, making it particularly useful in binary classification problems where the goal is to model probabilities.</p>
<p>The mathematical expression for the sigmoid activation function is as follows:</p>
<p>Sigmoid Activation: σ(x) = 1 / (1 + exp(-x))</p>
<p>In this equation:</p>
<ul>
<li>σ(x) represents the output of the sigmoid function.</li>
<li>x is the input value to the function.</li>
<li>&quot;exp&quot; stands for the exponential function.</li>
</ul>
<p>Key characteristics and applications of the sigmoid activation function include:</p>
<ol>
<li>
<p><strong>Output Range:</strong> The sigmoid function outputs values in the range of 0 to 1. This output range can be interpreted as a probability, which is particularly valuable in binary classification tasks. A value close to 1 indicates high confidence in one class, while a value close to 0 indicates high confidence in the other class.</p>
</li>
<li>
<p><strong>Sigmoid Curve:</strong> The shape of the sigmoid curve is an S-shaped curve. It's monotonically increasing and differentiable, which facilitates gradient-based optimization during the training of neural networks.</p>
</li>
<li>
<p><strong>Non-Linearity:</strong> Sigmoid introduces non-linearity to the network. This non-linearity allows neural networks to learn complex patterns and relationships in the data, making it suitable for various problems beyond classification, such as feature learning.</p>
</li>
</ol>
<p>However, there are some considerations when using sigmoid activation:</p>
<ul>
<li>
<p><strong>Vanishing Gradient:</strong> In deep neural networks, the gradient of the sigmoid function can become very small for extreme input values (very positive or very negative), which can lead to the vanishing gradient problem, making training difficult. To address this issue, other activation functions like ReLU (Rectified Linear Unit) and its variants have become more popular in deep learning.</p>
</li>
<li>
<p><strong>Centered Output:</strong> The sigmoid function produces centered outputs around 0.5, which may not be ideal in cases where the outputs need to cover a broader range. For instance, in regression problems, a different activation function like the hyperbolic tangent (tanh) may be preferred.</p>
</li>
</ul>
<p>Despite these limitations, the sigmoid activation function remains valuable in certain scenarios, especially in the output layer of binary classification models, where it's used to estimate probabilities and make decisions based on a threshold.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/KOhbp3EIRlM?si=5ez8PBKLwXx3XzUI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="29-normalized-device-coordinate-ndc-a-name%22ndc%22a">29. Normalized Device Coordinate (NDC) <a name="ndc"></a></h3>
<p>from ChatGPT, &quot;Normalized Device Coordinates (NDC) is a coordinate system used in computer graphics to represent the geometry of objects within the viewing volume or frustum. NDC is a common intermediate step in the rendering pipeline of 3D graphics, occurring after the transformation of 3D world coordinates into a 2D projection, but before the final transformation into pixel coordinates on a screen.</p>
<p>Here are the key characteristics of Normalized Device Coordinates:</p>
<ol>
<li>
<p><strong>Canonical Viewing Volume:</strong> NDC is defined within a canonical viewing volume that ranges from -1 to 1 in all three dimensions: X, Y, and Z. In this volume, the origin (0, 0, 0) corresponds to the center of the viewing frustum.</p>
</li>
<li>
<p><strong>Normalization:</strong> The process of converting 3D world coordinates (often in a Cartesian coordinate system) into NDC involves dividing each coordinate by the appropriate scale factor to bring it into the -1 to 1 range. This is typically done for both X, Y, and Z coordinates, resulting in a normalized point in NDC.</p>
</li>
<li>
<p><strong>Homogeneous Coordinates:</strong> NDC is often represented using homogeneous coordinates, which means that the coordinates are expressed as (X, Y, Z, W), where W is a scaling factor. In NDC, W is typically set to 1, so the coordinates are effectively (X, Y, Z, 1).</p>
</li>
<li>
<p><strong>Clipping and Projection:</strong> Objects and parts of objects that fall outside the -1 to 1 range in NDC are typically clipped, meaning they are discarded or transformed into smaller pieces that fit within the viewing volume. This clipping process ensures that only visible objects are rendered.</p>
</li>
<li>
<p><strong>Depth Information:</strong> The Z-coordinate in NDC represents the depth of a point in the viewing volume. Values closer to -1 are nearer to the viewer, while values closer to 1 are farther away.</p>
</li>
<li>
<p><strong>Screen Space Transformation:</strong> After the NDC stage, further transformation is applied to convert NDC coordinates into screen or pixel coordinates, taking into account the dimensions and resolution of the output display.</p>
</li>
</ol>
<p>Normalized Device Coordinates are an essential concept in computer graphics as they provide a standardized way to represent the visible geometry within the viewing frustum, making it easier to perform rendering operations like shading, lighting, and texturing. The NDC space serves as a common reference frame for rendering systems and graphics hardware.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Ck1SH7oYRFM?si=RoFjd1c1HI3tf6bC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="30-triangle-rasterization-a-name%22trirast%22a">30. Triangle rasterization <a name="trirast"></a></h3>
<p>from ChatGPT, &quot;The triangle rasterization pipeline, often referred to as the rasterization stage, is a fundamental component of the graphics rendering process in a computer graphics system, such as a graphics processing unit (GPU). It's responsible for converting geometric primitives, typically triangles, into individual pixels on the screen or image plane. The entire process can be broken down into several stages:</p>
<ol>
<li>
<p><strong>Vertex Transformation:</strong> The pipeline begins with the transformation of vertices from their original world or object space into normalized device coordinates (NDC). This transformation involves operations like modeling transformations, view transformations, and perspective division.</p>
</li>
<li>
<p><strong>Clipping:</strong> Before rasterization, vertices and primitives that are outside the viewing frustum (defined by NDC volume) are clipped or discarded. This ensures that only visible portions of the primitives are processed further.</p>
</li>
<li>
<p><strong>Triangle Setup:</strong> Once the vertices have been transformed and clipped, the pipeline determines the parameters needed to interpolate data across the triangle's surface. This includes calculating gradients for attributes like color, texture coordinates, and depth.</p>
</li>
<li>
<p><strong>Scan Conversion:</strong> This stage involves converting the triangle from its continuous, 2D representation into a set of discrete pixels (samples) that cover the triangle's surface. Each pixel within the triangle's bounding box is evaluated to determine whether it falls inside the triangle, and if it does, attributes are interpolated across the pixel.</p>
</li>
<li>
<p><strong>Attribute Interpolation:</strong> For each pixel within the triangle, the values of attributes (e.g., color, texture coordinates, depth) are interpolated based on the gradient values determined during the triangle setup stage.</p>
</li>
<li>
<p><strong>Depth Test:</strong> The depth values of each pixel are compared to the existing depth values in the frame buffer. If the new depth value is closer to the viewer, the pixel's attributes (e.g., color) are updated. This process ensures that only the visible surfaces of objects are drawn.</p>
</li>
<li>
<p><strong>Alpha Blending and Fragment Operations:</strong> After depth testing, pixel values are processed for alpha blending, which allows for the mixing of transparent or translucent objects. Fragment operations like blending and anti-aliasing may also be applied.</p>
</li>
<li>
<p><strong>Frame Buffer:</strong> The final pixel values are written to the frame buffer, which represents the image or scene as seen from the camera or viewer's perspective. This frame buffer can be displayed on a screen or further processed for additional rendering stages.</p>
</li>
</ol>
<p>The triangle rasterization pipeline is essential for rendering 3D scenes and objects on a 2D display. It forms the core of real-time graphics rendering in video games, 3D modeling software, and various other applications that require the conversion of 3D geometry into 2D images.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/k5wtuKWmV48?si=2wBwe2pk0bboGj3c" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="31-%22linear-in-disparity%22-a-name%22lid%22a">31. &quot;Linear in disparity&quot; <a name="lid"></a></h3>
<p>from ChatGPT, &quot;In the context of computer vision and stereo vision, &quot;linear in disparity&quot; refers to the relationship between depth or disparity values and the corresponding image coordinates in a scene. Disparity, in this context, represents the difference in the horizontal positions of corresponding points in the left and right images of a stereo pair and is used to calculate depth information.</p>
<p>When a relationship is described as &quot;linear in disparity,&quot; it means that the depth or disparity values change linearly with respect to the position in the image. This implies that, for small disparities, the depth can be approximated as a linear function of disparity, which simplifies the stereo vision process.</p>
<p>For example, if the disparity between two points in the left and right images is D pixels, and the baseline (the distance between the camera positions for the left and right images) is B meters, then the depth (Z) of the point can be estimated as follows in a linear relationship:</p>
<p>Z = (B * focal length) / D</p>
<p>In this equation:</p>
<ul>
<li>Z is the depth of the point in the scene.</li>
<li>B is the baseline, representing the distance between the two camera positions.</li>
<li>The focal length represents the camera's focal length in pixels.</li>
</ul>
<p>This linear relationship between depth and disparity holds when the baseline is small relative to the focal length and the depth variation in the scene is also relatively small.</p>
<p>Understanding this linear relationship is fundamental for tasks such as stereo vision, 3D reconstruction, and depth mapping in computer vision applications. It simplifies the process of estimating the depth information from stereo images and provides a basis for generating 3D reconstructions from 2D images.&quot;</p>
<h3 id="32-%22rays-to-map-them-from-camera-space-to-ndc-space%22-a-name%22raymap%22a">32. &quot;rays to map them from camera space to NDC space&quot; <a name="raymap"></a></h3>
<p>from ChatGPT, &quot;The phrase &quot;rays to map them from camera space to NDC space&quot; describes a critical process in computer graphics and computer vision, particularly in rendering and 3D scene representation. Here's a more detailed explanation of each part of the phrase:</p>
<ol>
<li>
<p><strong>Rays:</strong> In the context of computer graphics and computer vision, &quot;rays&quot; typically refer to lines or vectors that are used to represent the paths of light or the lines of sight from a camera or observer to points in a 3D scene. Rays are essential for tasks like rendering and 3D reconstruction because they help determine what is visible from the camera's perspective.</p>
</li>
<li>
<p><strong>Map:</strong> In this context, &quot;map&quot; means to transform or convert data from one coordinate system to another. It involves taking the information associated with the rays (such as their positions and directions) and re-representing it in a different coordinate system.</p>
</li>
<li>
<p><strong>Camera Space:</strong> &quot;Camera space&quot; refers to a coordinate system or 3D space that is centered around the camera's viewpoint. The camera's position is usually located at the origin, and the optical axis (the direction the camera is pointing) often aligns with the Z-axis. In camera space, the positions and directions of rays are described relative to the camera.</p>
</li>
<li>
<p><strong>NDC Space:</strong> &quot;NDC&quot; stands for Normalized Device Coordinates, and it is a standardized coordinate system used in computer graphics to represent the geometry of objects and scenes within the viewing frustum or volume. In NDC space, the viewing volume is typically defined within a cube or rectangular volume ranging from -1 to 1 along the X, Y, and Z axes. NDC space is used for consistency in rendering and further computational steps.</p>
</li>
</ol>
<p>The process of &quot;mapping rays from camera space to NDC space&quot; involves taking rays that originate from the camera's perspective and transforming their positions and directions so that they are represented within the NDC coordinate system. This mapping is essential for consistency and alignment between the camera's viewpoint, the scene geometry, and the subsequent rendering or computational steps that take place in NDC space.</p>
<p>In summary, this process is a fundamental step in rendering and 3D computer vision applications, ensuring that the data and rays are in the same coordinate system, allowing for accurate rendering, visualization, and computation of 3D scenes.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/pThw0S8MR7w?si=-2MJLLxybY5-QbLS" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="33-original-viewing-frustum-a-name%22ovf%22a">33. Original viewing frustum <a name="ovf"></a></h3>
<p>from ChatGPT, &quot;An &quot;original viewing frustum&quot; typically refers to the initial or unaltered viewing frustum in computer graphics, which represents the volume or space within which a camera or viewer can see objects in a 3D scene. The term &quot;frustum&quot; describes the shape of the viewing volume, which is pyramid-like with its top cut off, resulting in a truncated pyramid.</p>
<p>Here are the key components and characteristics of the original viewing frustum:</p>
<ol>
<li>
<p><strong>Geometry:</strong> The original viewing frustum is defined by six planes that form the boundaries of the volume. These planes are typically defined as the near plane, far plane, left plane, right plane, top plane, and bottom plane. The near plane is the closest to the camera, while the far plane is the farthest from the camera. The other four planes define the sides of the frustum.</p>
</li>
<li>
<p><strong>Field of View (FOV):</strong> The shape and size of the frustum depend on the camera's field of view. A wider field of view results in a larger frustum, allowing the camera to capture a wider area of the 3D scene.</p>
</li>
<li>
<p><strong>Aspect Ratio:</strong> The aspect ratio of the frustum is determined by the dimensions of the output display (e.g., screen or viewport). It influences the shape of the frustum, ensuring that objects are not distorted when projected onto the screen.</p>
</li>
<li>
<p><strong>Depth Range:</strong> The frustum has a depth range defined by the near and far planes. Objects that fall within this depth range are considered within the camera's view, and those outside it are not visible.</p>
</li>
</ol>
<p>The original viewing frustum serves as a fundamental concept in computer graphics and rendering. It helps determine which objects in a 3D scene are visible from the camera's perspective and need to be rendered. It also plays a crucial role in tasks like view frustum culling, which optimizes rendering by discarding objects that are entirely outside the frustum and thus not visible to the camera.</p>
<p>In the rendering pipeline, objects are typically transformed into the viewing frustum's space and then projected onto the 2D screen space for rendering. The original viewing frustum provides the initial boundaries for this projection, and it can be adjusted or transformed as needed for various rendering and visualization tasks.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/GqEP79loyQE?si=2Tj-b2pjmLmjfg38" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="34-standard-pinhole-camera-model-a-name%22spcm%22a">34. Standard pinhole camera model <a name="spcm"></a></h3>
<p>from ChatGPT, &quot;The standard pinhole camera model is a simplified mathematical representation of how a camera captures images of the 3D world in 2D space. It provides a basic framework for understanding the fundamental concepts of perspective and camera geometry in computer vision and computer graphics.</p>
<p>The model assumes a simple, idealized camera with the following components:</p>
<ol>
<li>
<p><strong>Pinhole:</strong> At the center of the camera, there is a tiny aperture or pinhole through which light rays from the 3D scene pass to form an image on the camera's imaging plane. This pinhole is the &quot;lens&quot; of the idealized camera and serves as the point where all incoming rays converge.</p>
</li>
<li>
<p><strong>Imaging Plane:</strong> The imaging plane is positioned some distance behind the pinhole and acts as the surface on which the image is formed. It's often referred to as the image sensor or film plane in real cameras.</p>
</li>
<li>
<p><strong>Focal Length (f):</strong> The distance from the pinhole to the imaging plane is known as the focal length (denoted as &quot;f&quot; in the model). It determines the perspective and magnification of objects in the image.</p>
</li>
</ol>
<p>Key principles of the standard pinhole camera model include:</p>
<ul>
<li>
<p><strong>Perspective Projection:</strong> The model is based on the principle of perspective projection, where light rays from the 3D scene converge at the pinhole and then project onto the imaging plane. This projection causes objects closer to the camera to appear larger in the image, and those farther away to appear smaller.</p>
</li>
<li>
<p><strong>Inverted Image:</strong> The resulting image is an inverted representation of the scene, meaning that objects appear upside down in the image compared to their orientation in the 3D world. This inversion occurs because the light rays cross at the pinhole before reaching the imaging plane.</p>
</li>
<li>
<p><strong>Pinhole Camera Equation:</strong> The pinhole camera model is described by the pinhole camera equation, which relates the 3D coordinates of a point in the scene (X, Y, Z) to its 2D image coordinates (x, y) using focal length and the camera's position and orientation.</p>
</li>
<li>
<p><strong>No Lens Distortion:</strong> The standard pinhole camera model does not account for lens distortion, which is a real-world phenomenon in cameras. Lens distortion can cause image imperfections like radial distortion and tangential distortion.</p>
</li>
</ul>
<p>While the standard pinhole camera model is a simplification and idealization of real camera systems, it forms the foundation for understanding how cameras capture images and the principles of geometric projection. It is a fundamental concept in computer vision, computer graphics, and photogrammetry, serving as a starting point for more complex camera models and calibration techniques used in various applications.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/_EhY31MSbNM?si=RvJtcyma66zq8qup" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h3 id="35-texture-mapped-meshes-a-name%22tmm%22a">35. Texture-mapped meshes <a name="tmm"></a></h3>
<p>from ChatGPT, &quot;Texture-mapped meshes are 3D models used in computer graphics and 3D computer visualization where a texture or image is applied to the surface of a 3D mesh to provide details, colors, or patterns. The process of applying a 2D image to a 3D object is known as &quot;texture mapping.&quot; Texture mapping allows you to make 3D models appear more realistic, detailed, and visually appealing.</p>
<p>Here are the key components and concepts related to texture-mapped meshes:</p>
<ol>
<li>
<p><strong>3D Mesh:</strong> A 3D mesh is a collection of vertices, edges, and faces that defines the geometry and structure of a 3D object. These meshes are used to create a wide range of 3D models, from simple shapes to complex characters and environments.</p>
</li>
<li>
<p><strong>Texture Mapping:</strong> Texture mapping involves taking a 2D image (the texture) and applying it to the surface of a 3D mesh. The process includes mapping the 2D image onto the 3D geometry in a way that aligns the image with the object's surface. This provides the appearance of detail, color, or patterns on the object.</p>
</li>
<li>
<p><strong>Texture Coordinates (UV Mapping):</strong> To apply a 2D texture to a 3D mesh, each vertex of the mesh is associated with specific texture coordinates, typically denoted as (u, v). These coordinates determine how the 2D image is wrapped or projected onto the 3D surface. The process of assigning texture coordinates to vertices is known as UV mapping.</p>
</li>
<li>
<p><strong>Types of Textures:</strong> Textures can vary in content and purpose. They can be simple color maps to add color and shading to the object, normal maps to simulate surface details and lighting effects, bump maps to create the illusion of small-scale surface features, or even image-based textures that depict complex patterns or scenes.</p>
</li>
<li>
<p><strong>Realism and Detail:</strong> Texture mapping enhances the realism of 3D models. It allows artists and developers to add intricate details, such as skin pores, fabric patterns, or weathering, to make objects appear more lifelike.</p>
</li>
<li>
<p><strong>Efficiency:</strong> Texture mapping is an efficient way to add visual detail to 3D models without increasing the complexity of the geometry. This is crucial for real-time applications like video games and interactive simulations.</p>
</li>
<li>
<p><strong>Shader Programs:</strong> In modern graphics programming, texture-mapped meshes are often used in conjunction with shader programs to apply complex materials and lighting effects to the textured surfaces, further enhancing the realism of the 3D objects.</p>
</li>
</ol>
<p>Texture-mapped meshes are a fundamental concept in computer graphics and 3D rendering, playing a crucial role in creating visually appealing and realistic 3D scenes and objects. They are used in a wide range of applications, from video games and animation to architectural visualization and virtual reality.&quot;</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/HRNIWK5CCak?si=B8_1CrfpA70qNANR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>
</body>
</html>
